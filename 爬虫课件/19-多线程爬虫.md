- 线程池爬虫

  ```python
  from concurrent.futures import ThreadPoolExecutor
  from concurrent.futures import as_completed
  import requests
  
  url = 'https://www.baidu.com/s?wd=%E7%BE%8E%E5%A5%B3&pn='
  
  
  # 发起请求
  def request(url):  # 可以用不定长参数
      print(url)
      response = requests.get(url)
      return response
  
  
  def parse(result):
      '''
      解析
      :param result:
      :return:
      '''
      return ['https://www.baidu.com/']  # 返回新的url
  
  
  def main():
      with ThreadPoolExecutor(max_workers=28) as executor:
          url_list = []  # 装URL的列表
          for i in range(1, 11):  # 一共发起10也
              full_url = url + str((i - 1) * 10)
              url_list.append(full_url)
          result = executor.map(request, url_list)
          for res in result:
              new_url = parse(res)  # 去解析
              result1 = executor.map(request, new_url)  # 继续请求
              for res1 in result1:
                  print(res1)
  
      # 第二种
      '''
      
      with ThreadPoolExecutor(max_workers=28) as executor:
        future_list = []
        for i in range(1, 11):  # 一共发起10也
              full_url = url + str((i - 1) * 10)
              future= executor.submit(request, full_url)
              future_list.append(future)
      for res in as_completed(futrue_list): 
          print(res.result())              
      '''
  
  
  if __name__ == '__main__':
      main()
  
  ```

- 进程池爬虫

  ```
  from concurrent.futures import ProcessPoolExecutor
  from concurrent.futures import as_completed
  import requests
  
  url = 'https://www.baidu.com/s?wd=%E7%BE%8E%E5%A5%B3&pn='
  
  
  # 发起请求
  def request(url):  # 可以用不定长参数
      print(url)
      response = requests.get(url)
      return response
  
  
  def parse(result):
      '''
      解析
      :param result:
      :return:
      '''
      return ['https://www.baidu.com/']  # 返回新的url
  
  
  def main():
      with ProcessPoolExecutor(max_workers=3) as executor:
          url_list = []  # 装URL的列表
          for i in range(1, 11):  # 一共发起10也
              full_url = url + str((i - 1) * 10)
              url_list.append(full_url)
          result = executor.map(request, url_list)
          for res in result:
              print(res)
              new_url = parse(res)  # 去解析
              result1 = executor.map(request, new_url)  # 继续请求
              for res1 in result1:
                  print(res1)
      '''第二种
      with ProcessPoolExecutor(max_workers=3) as executor:
          future_list = []
          for i in range(1, 11):  # 一共发起10也
              full_url = url + str((i - 1) * 10)
              future = executor.submit(request, full_url)
              future_list.append(future)
          for res in as_completed(future_list):
              print(res.result())
      '''
  
  
  if __name__ == '__main__':
      main()
  
  ```

  

- 多协程爬虫

  ```
  import requests
  import gevent
  from gevent import monkey
  from gevent.pool import Pool
  
  #把当前的IO操作，打上标记，以便于gevent能检测出来实现异步(否则还是串行）
  monkey.patch_all()
  
  
  def task(url):
      '''
      1、request发起请求
      :param url:
      :return:
      '''
      response = requests.get(url)
      print(response.status_code)
      
      
  #控制最多一次向远程提交多少个请求，None代表不限制
  pool = Pool(5)
  gevent.joinall([
      pool.spawn(task,url='https://www.baidu.com'),
      pool.spawn(task,url='http://www.sina.com.cn'),
      pool.spawn(task,url='https://news.baidu.com'),
  ])
  
  gevent+reqeust+Pool（控制每次请求数量）
  ```

  

